# -*- coding: utf-8 -*-
"""BERT_Text_pre_processing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wmtyjgMZWGOHT-_S7WqPR8RUc8GodVje

Useful Article on BERT word-embeddings: https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/
"""

#import libraries
import pandas as pd
import torch
from transformers import BertTokenizer, BertModel, BertForTokenClassification
import numpy as np
import nltk
import re
from stop_words import get_stop_words
from nltk.stem import WordNetLemmatizer
from rank_bm25 import BM25Okapi
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')

#Functions

#Function that checks if an ADR is in text
def is_included(row):
    list1, list2 = row['tokenized'], row['tokenized_adr']
    result = []

    i = 0
    while i < len(list1):

        if list1[i:i+len(list2)] == list2:
          for j in range(len(list2)):
            result.append(1)
            i += 1
        else:
            result.append(0)
            i += 1
            
    return result


# define a function to sum the lists
def sum_lists(lst):
    return [sum(x) for x in zip(*lst)]



#convert to BERT format
def convert_BERT_format(df_1, column):

  # add the symbol [CLS] as the beginning of each sentence
  df_1[column] = df_1.apply(lambda x: '[CLS]' + ' ' + str(x[column]), axis = 1)
  # add the symbol [SEP] at the end of each sentence in the review 
  df_1[column] = df_1.apply(lambda x: str(x[column]).replace('.', '. [SEP]'), axis = 1)

  return df_1


def remove_whitespace(df_1, column):
  
  #remove \n and \t (for some reason we have to do this when we load back in the data)
  df_1[column] = df_1.apply(lambda x: str(x[column]).replace("\n", " "), axis = 1)
  df_1[column] = df_1.apply(lambda x: str(x[column]).replace("\t", " "), axis = 1)


def preprocessing(content, remove_sw):
    # convert the text to lowercase
    content = content.lower()
    regex = re.compile('[^a-z\s]+')

    # remove all commas so that constructions such as $70,000 maintain their meaning and do not get split:'70', '000'
    content = regex.sub('', content)

    # https://www.adamsmith.haus/python/answers/how-to-remove-all-punctuation-marks-with-nltk-in-python
    # remove punctuation and tokenize (which will be the same as 1-grams)
    tokenizer = nltk.RegexpTokenizer(r"\w+")
    one_grams = tokenizer.tokenize(content)

    #remove stopwords
    if remove_sw == True:
        one_grams = [i for i in one_grams if i not in get_stop_words('english')]

    # lemmatize
    lemmatizer = WordNetLemmatizer()
    words = []
    for word in one_grams:
        words.append(lemmatizer.lemmatize(word))   

    return words


def add_labels(df_1, df_2, tokenizer_type, column, id_column, symptom, remove_stopw):
  '''
  A function that geiven input of a review and symptoms, returns a preprocessed data set
  where we have the review tokenized, and a corresponding columns that shows at which position
  an ADR can be dfound. For examle if we have "My myscles hurt", the returned value would be
  one column ['my', 'muslces', 'hurt'] and another column [0,1,0] with the 1 indicating the position
  of the ADR
  Inputs:
        df_1 - dataframe which contains the reviwes (dataframe)
        df_2 - a dataframe that contains the ADRs (dataframe)
        tokenizer_type - input "BERT" for bert tokenizer and "Other" for regular (str)
        column - the column in which the text review is found in df_1 dataset (str)
        id_column - the column by which df_1 and df_2 can be merged (str)
        symptom - the name of the column which contains the AD in df_2 (str)
        remove_stopw - set True to remove and False not to remove (boolen)
                      Note: for BERT, we never remove the stop words
  Output:
        preprocess_data - the datframe whith the processed reviews, and correposing
        location of the ADR (dataframe)
  '''
  #remove white space
  remove_whitespace(df_1, column)
  
  #BERT Tokenizer
  if tokenizer_type == "BERT":
    convert_BERT_format(df_1, column)

    # Load pre-trained model tokenizer (vocabulary)
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

    df_1['tokenized'] = df_1.apply(lambda x: tokenizer.tokenize(x[column]), axis =1)
    df_2['tokenized_adr'] = df_2.apply(lambda x: tokenizer.tokenize(str(x[symptom])), axis =1)

  #Non-BERT tokenizer
  else:

    
    #convert the columns to list
    df_1_list = df_1[column].to_list()
    df_2_list = df_2[symptom].to_list()

    #preprocess the text
    preprocessed_1 = [preprocessing(str(i), remove_sw=remove_stopw) for i in df_1_list]
    preprocessed_2 = [preprocessing(str(i), remove_sw=remove_stopw) for i in df_2_list]

    df_1['tokenized'] = preprocessed_1
    df_2['tokenized_adr'] = preprocessed_2


  #merge the 2 dataframes
  merged_df = df_1.merge(df_2, on= id_column, how='left')
  merged_df = merged_df[merged_df['tokenized_adr'].notna()]

  merged_df['token_included'] = merged_df.apply(is_included, axis=1)

  # group the dataframe by "group_col" and apply the "sum_lists" function to "Tokens"
  result = merged_df.groupby(id_column)['token_included'].apply(sum_lists).reset_index()

  # rename the columns in the result dataframe
  result.columns = [id_column, 'pre_processed_tokens']

  #merge back with the rest of the data to get final output
  preprocess_data = df_1.merge(result, on=id_column, how='left')
  preprocess_data = preprocess_data[preprocess_data['pre_processed_tokens'].notna()]
  
  #replace all values higher than 1 with 0
  list_tokenized =  preprocess_data['pre_processed_tokens'].to_list()
  for i in list_tokenized:
    for j in range(len(i)):

      if i[j] > 1: 
        i[j] = 1

  return preprocess_data




class form_input():
    def __init__(self,ID, sentence, label, data_type='train'):
        self.sentence = sentence
        self.id = ID
        self.label = label
        self.max_length = 400
        #self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased').convert_tokens_to_ids(toks)
        self.data_type = data_type
    
    def __len__(self):
        return len(self.sentence)
    
    def __getitem__(self, item):
        toks = self.sentence[item]
        label = self.label[item]
        
        if len(toks)>self.max_length:
          toks = toks[:self.max_length]
          label = label[:self.max_length]

        ids = BertTokenizer.from_pretrained('bert-base-uncased').convert_tokens_to_ids(toks)
        tok_type_id = [0] * len(ids)
        att_mask = [1] * len(ids)

         # Padding
        pad_len = self.max_length - len(ids)        
        ids = ids + [2] * pad_len
        tok_type_id = tok_type_id + [0] * pad_len
        att_mask = att_mask + [0] * pad_len
        
        
        ########################################            
        # Forming the label
        if self.data_type == 'test':
          label = 1
        else:
          label = label  + [2]*pad_len
    
        return {'ids': torch.tensor(ids, dtype = torch.long),
                'tok_type_id': torch.tensor(tok_type_id, dtype = torch.long),
                'att_mask': torch.tensor(att_mask, dtype = torch.long),
                'target': torch.tensor(label, dtype = torch.long)
               }

def dataset_2_list(df):
    id_list = df['txt_id'].values.tolist()
    sentences_list = df['tokenized'].values.tolist()
    #keywords_list = df.kword.apply(lambda x : eval(x)).values.tolist()
    
    labels_list = df['pre_processed_tokens'].values.tolist()    
    
    return id_list, sentences_list, labels_list


def train(data_loader, epochs, device):
    model = BertForTokenClassification.from_pretrained('bert-base-uncased',  num_labels = 3).to(device)
    optimizer = torch.optim.Adam(params = model.parameters(), lr=3e-5)
    model.trainable = False

    for epoch in range(epochs):
      for index, dataset in enumerate(data_loader):
          batch_input_ids = dataset['ids'].to(device, dtype = torch.long)
          batch_att_mask = dataset['att_mask'].to(device, dtype = torch.long)
          batch_tok_type_id = dataset['tok_type_id'].to(device, dtype = torch.long)
          batch_target = dataset['target'].to(device, dtype = torch.long)
          #if batch_input_ids.size() != batch_target.size():
            #continue
          
          #try:
          output = model(batch_input_ids, 
                         token_type_ids=None,
                          attention_mask=batch_att_mask,
                          labels=batch_target)
          #except RuntimeError:
            #continue
            
          
          loss = output[0]
          prediction = output[1]
          
          loss.sum().backward()
          optimizer.step()        
          #train_loss +=
          optimizer.zero_grad()
      print('Epoch {}: Loss = {}'.format(epoch, loss.item()))

    return model


def eval_fn(data_loader, model):
    '''
    Functiont to evaluate the model on each epoch. 
    We can also use Jaccard metric to see the performance on each epoch.
    '''
    
    model.eval()
    
    eval_loss = 0
    predictions = np.array([], dtype = np.int64).reshape(0, config['MAX_LEN'])
    true_labels = np.array([], dtype = np.int64).reshape(0, config['MAX_LEN'])
    
    with torch.no_grad():
        for index, dataset in enumerate(tqdm(data_loader, total = len(data_loader))):
            batch_input_ids = dataset['ids'].to(config['device'], dtype = torch.long)
            batch_att_mask = dataset['att_mask'].to(config['device'], dtype = torch.long)
            batch_tok_type_id = dataset['tok_type_id'].to(config['device'], dtype = torch.long)
            batch_target = dataset['target'].to(config['device'], dtype = torch.long)

            output = model(batch_input_ids, 
                           token_type_ids=None,
                           attention_mask=batch_att_mask,
                           labels=batch_target)

            step_loss = output[0]
            eval_prediction = output[1]

            eval_loss += step_loss
            
            eval_prediction = np.argmax(eval_prediction.detach().to('cpu').numpy(), axis = 2)
            actual = batch_target.to('cpu').numpy()
            
            predictions = np.concatenate((predictions, eval_prediction), axis = 0)
            true_labels = np.concatenate((true_labels, actual), axis = 0)
            
    return eval_loss.sum(), predictions, true_labels


